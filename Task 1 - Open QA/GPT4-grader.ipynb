{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["SHwTFSRPFTyr"],"mount_file_id":"1Fc2NYYrnYl84gVyRudCnLKP8VR5SZtxz","authorship_tag":"ABX9TyNwuK6aym5XunsdFqR75A4Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qbpf3tTa0_-_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702858888767,"user_tz":-540,"elapsed":8308,"user":{"displayName":"‍정지원[ 학부재학 / 데이터과학과 ]","userId":"01067494734646856617"}},"outputId":"0fcf0458-f9be-4862-ef89-47ffc5ae3b40"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting openai\n","  Downloading openai-1.5.0-py3-none-any.whl (223 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/223.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/223.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.7/223.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n","Collecting httpx<1,>=0.23.0 (from openai)\n","  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n","Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n","  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.5.0\n"]}],"source":["!pip install --upgrade openai"]},{"cell_type":"code","source":["import openai"],"metadata":{"id":"kTB4UR53H5GD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## One prompt example"],"metadata":{"id":"KNs1SPf7WFo0"}},{"cell_type":"code","source":["prompt1 = open(\"prompt1.txt\").read()\n","print(prompt1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TU3eWXQGI8aH","executionInfo":{"status":"ok","timestamp":1700324464932,"user_tz":-540,"elapsed":5,"user":{"displayName":"‍정지원[ 학부재학 / 데이터과학과 ]","userId":"01067494734646856617"}},"outputId":"88577ed0-159e-470f-84fe-954e9b7f684a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["You will be given one question and one answer. Your task is to rate the answer on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n","\n","Evaluation Criteria:\n","Factual Consistency (1-5): Does the answer contain untruthful or misleading facts that are not supported by the question?\n","\n","Evaluation Steps:\n","1. Review the question and answer provided carefully to understand the context.\n","2. Evaluate the given answer's factual consistency based on whether the information aligns with or contradicts the question.\n","3. Rate the factual consistency of the answer on a scale of 1 to 5, where 1 indicates a low level and 5 indicates a high level of consistency with the question.\n","\n","Example:\n","\n","Question:\n","\n","{{Question}}\n","\n","Answer:\n","\n","{{Answer}}\n","\n","Evaluation Form (scores ONLY):\n","\n","- Factual Consistency:\n"]}]},{"cell_type":"code","source":["q_input = \"If your dataset is suffering from high variance, how would you handle it?\"\n","a_output = \"High variance in a dataset is a common problem that can lead to inaccurate predictions. There are several ways to handle high variance in a dataset, but one of the most effective methods is to use a regularization technique such as L1 or L2 regularization. This method can help reduce the variance in the dataset by adding a penalty term to the cost function.\\n\\nAnother approach is to use a different cost function such as the hinge loss function or the logistic loss function. These cost functions are more robust to high variance than the traditional cost function and can lead to more accurate predictions.\\n\\nFinally, you can use a combination of these techniques to handle high variance in a dataset. For example, you can use regularization techniques to reduce the variance in the dataset and then use a different cost function to improve the accuracy of the predictions.\""],"metadata":{"id":"dSTE4EChPOJT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cur_prompt = prompt1.replace('{{Question}}', q_input).replace('{{Answer}}', a_output)\n","print(cur_prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HS7pVutLPNbX","executionInfo":{"status":"ok","timestamp":1700326429325,"user_tz":-540,"elapsed":5,"user":{"displayName":"‍정지원[ 학부재학 / 데이터과학과 ]","userId":"01067494734646856617"}},"outputId":"f27418a4-5e3b-447e-cb0c-862cc35e433c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["You will be given one question and one answer. Your task is to rate the answer on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n","\n","Evaluation Criteria:\n","Factual Consistency (1-5): Does the answer contain untruthful or misleading facts that are not supported by the question?\n","\n","Evaluation Steps:\n","1. Review the question and answer provided carefully to understand the context.\n","2. Evaluate the given answer's factual consistency based on whether the information aligns with or contradicts the question.\n","3. Rate the factual consistency of the answer on a scale of 1 to 5, where 1 indicates a low level and 5 indicates a high level of consistency with the question.\n","\n","Example:\n","\n","Question:\n","\n","If your dataset is suffering from high variance, how would you handle it?\n","\n","Answer:\n","\n","High variance in a dataset is a common problem that can lead to inaccurate predictions. There are several ways to handle high variance in a dataset, but one of the most effective methods is to use a regularization technique such as L1 or L2 regularization. This method can help reduce the variance in the dataset by adding a penalty term to the cost function.\n","\n","Another approach is to use a different cost function such as the hinge loss function or the logistic loss function. These cost functions are more robust to high variance than the traditional cost function and can lead to more accurate predictions.\n","\n","Finally, you can use a combination of these techniques to handle high variance in a dataset. For example, you can use regularization techniques to reduce the variance in the dataset and then use a different cost function to improve the accuracy of the predictions.\n","\n","Evaluation Form (scores ONLY):\n","\n","- Factual Consistency:\n"]}]},{"cell_type":"markdown","source":["### evaluation"],"metadata":{"id":"_k1xKE9_Yd8I"}},{"cell_type":"markdown","source":["[Parameters in chat.completions.create](https://platform.openai.com/docs/api-reference/chat/create)"],"metadata":{"id":"BVwm9gWrUIXf"}},{"cell_type":"code","source":["from openai import OpenAI\n","\n","client = OpenAI(\n","  api_key = \"api_key\"\n",")\n","\n","response = client.chat.completions.create(\n","    model = \"gpt-4-1106-preview\",\n","    messages = [{\"role\": \"system\", \"content\": cur_prompt}],\n","    temperature = 2,\n","    max_tokens = 10,\n","    top_p = 1, # defaults\n","    stop = None,\n","    n = 10\n",")"],"metadata":{"id":"v-ZSSkR3Pv8-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["0.003\\$ ~ 0.004\\$ 사용함"],"metadata":{"id":"H9ijCfHkVfUJ"}},{"cell_type":"code","source":["for r in response.choices:\n","  print(r.message.content)\n","  print('---')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rOUPg5KlUkCO","executionInfo":{"status":"ok","timestamp":1700326649992,"user_tz":-540,"elapsed":4,"user":{"displayName":"‍정지원[ 학부재학 / 데이터과학과 ]","userId":"01067494734646856617"}},"outputId":"4480aeb5-8480-4e4c-830f-0a9a74f0fdac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["4\n","---\n","Factual Consistency: 5\n","---\n","5\n","---\n","5\n","---\n","Factual Consistency: 5\n","\n","All the\n","---\n","5\n","---\n","- Factual Consistency: 5\n","---\n","5\n","---\n","5\n","---\n","Not applicable. (Note to editor superclass: Lia\n","---\n"]}]},{"cell_type":"markdown","source":["### result"],"metadata":{"id":"Kilpsa7NYgEn"}},{"cell_type":"code","source":["import re\n","\n","scores = []\n","for r in response.choices:\n","  scores += re.findall(r'\\d+', r.message.content)\n","\n","scores = [int(s) for s in scores]\n","print(round(sum(scores)/len(scores), 3))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sb1G4IakVtjE","executionInfo":{"status":"ok","timestamp":1700326913210,"user_tz":-540,"elapsed":261,"user":{"displayName":"‍정지원[ 학부재학 / 데이터과학과 ]","userId":"01067494734646856617"}},"outputId":"98c275e3-852f-4dc3-9976-ccb3693573d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["4.889\n"]}]},{"cell_type":"markdown","source":["## Load the results of LLaMA"],"metadata":{"id":"A0Otwhoz4EDr"}},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/Colab Notebooks/DATA410/Projects'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wUQZTJep6DqS","executionInfo":{"status":"ok","timestamp":1702222140892,"user_tz":-540,"elapsed":498,"user":{"displayName":"‍정지원[ 학부재학 / 데이터과학과 ]","userId":"01067494734646856617"}},"outputId":"a1b477ec-4490-41c1-c934-91d3ba1d3397"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/DATA410/Projects\n"]}]},{"cell_type":"code","source":["import json\n","\n","# JSONL 파일 읽기\n","file_path = 'llama2_got_results.jsonl' # JSONL 파일 경로\n","data = []  # JSON 객체들을 저장할 리스트\n","\n","with open(file_path, 'r', encoding='utf-8') as file:\n","    for line in file:\n","        # 각 줄마다 JSON 객체로 로드하여 리스트에 추가\n","        json_obj = json.loads(line.strip())\n","        data.append(json_obj)"],"metadata":{"id":"1M_DRSgk7VeA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(data[0])"],"metadata":{"id":"Q2z7S-FoNo7n","executionInfo":{"status":"ok","timestamp":1702222164387,"user_tz":-540,"elapsed":400,"user":{"displayName":"‍정지원[ 학부재학 / 데이터과학과 ]","userId":"01067494734646856617"}},"outputId":"571ac950-1ae2-4281-fe7b-8658976f0290","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'input': 'What is the purpose of Hash tables in statistics?', 'output': '\\n\\nHash tables are a fundamental data structure in computer science, and they have several purposes in statistics. A hash table, also known as a dictionary or map, is a data structure that stores key-value pairs in a way that allows for fast lookups, insertions, and deletions. In statistics, hash tables can be used in various ways, including:\\n\\n1. Association rule learning: Hash tables can be used to represent the items in a dataset and their associations. For example, a hash table can be used to store the items in a shopping cart and their frequencies. By analyzing the associations between items, statistical techniques such as frequent itemset mining can be used to identify popular combinations of items.\\n2. Clustering: Hash tables can be used to represent the similarity between data points. By computing the similarity between data points using a distance metric, such as Euclidean distance, the hash table can be used to group similar points together into clusters.\\n3. Dimensionality reduction: Hash tables can be used to reduce the dimensionality of a dataset by projecting the data onto a lower-dimensional space. For example, a hash table can be used to store the principal components of a dataset, which can be used to reduce the dimensionality of the data while preserving the most important information.\\n4. Data compression: Hash tables can be used to compress data by storing only the most frequent items in a dataset. By using a hash table to store the frequency of each item, the number of items that need to be stored can be significantly reduced, resulting in a more compact representation of the data.\\n5. Time series analysis: Hash tables can be used to store time series data and perform statistical analysis on it. For example, a hash table can be used to store the values of a time series at different time steps, and statistical techniques such as autocorrelation and moving average can be used to analyze the time series.\\n\\nIn summary, hash tables are a powerful tool in statistics, and they can be used in various ways to represent and analyze data. Their ability to efficiently store and retrieve data makes them an essential component of many statistical techniques.'}\n"]}]},{"cell_type":"code","source":["def export_qa(text):\n","  qa = text.split('Question:')[1].strip().split('Answer and Explanation:')\n","  q = qa[0].strip().strip('#').strip()\n","  a = qa[1].strip().strip('#').strip().replace('        ', '')\n","\n","  return (q, a)\n","\n","QAs = []\n","for output in data:\n","  output = output['output']\n","  qa = export_qa(output)\n","  QAs.append(qa)"],"metadata":{"id":"K1RCgrgPAkxv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Comparison on prompts"],"metadata":{"id":"SHwTFSRPFTyr"}},{"cell_type":"code","source":["import random\n","\n","sampled_QAs = random.sample(QAs, 5)\n","for (q, a) in sampled_QAs:\n","  print('Question:', q)\n","  print('Answer:', a)\n","  print('-----')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4yDzqUqUEymN","executionInfo":{"status":"ok","timestamp":1700546389555,"user_tz":-540,"elapsed":505,"user":{"displayName":"‍정지원[ 학부재학 / 데이터과학과 ]","userId":"01067494734646856617"}},"outputId":"794ed420-fd2f-4da7-e7a1-c82065952abf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Question: What is artificial intelligence?\n","Answer: 0 Artificial intelligence (AI) is the field of study concerned with creating intelligent machines that can perform tasks that typically require human intelligence, such as learning, problem-solving, and decision-making. AI is a multidisciplinary field that combines computer science, statistics, and engineering to create intelligent systems that can be applied to various industries, such as healthcare, finance, and transportation.\n","\n","There are several subfields within AI, including:\n","\n","1. Machine learning: This subfield focuses on developing algorithms and statistical models that enable machines to learn from data and make predictions or decisions without being explicitly programmed.\n","2. Natural language processing (NLP): This subfield deals with the interaction between computers and humans through the use of language, allowing machines to understand, interpret, and generate human language.\n","3. Computer vision: This subfield involves the development of algorithms and models that enable machines to interpret and understand visual data from the world around us, such as images and videos.\n","4. Robotics: This subfield focuses on the design and development of robots that can perform tasks that typically require human intelligence, such as manipulation and locomotion.\n","\n","AI has numerous applications across various industries, including:\n","\n","1. Healthcare: AI can be used to analyze medical images, diagnose diseases, and develop personalized treatment plans.\n","2. Finance: AI can be used to detect fraud, analyze financial data, and make investment decisions.\n","3. Transportation: AI can be used to develop autonomous vehicles that can navigate and make decisions without human intervention.\n","4. Education: AI can be used to personalize learning experiences, grade assignments, and develop virtual teaching assistants.\n","\n","Despite its many benefits, AI also raises several challenges and concerns, including:\n","\n","1. Bias and discrimination: AI systems can perpetuate existing biases and discrimination if they are trained on biased data.\n","2. Privacy: AI systems often require access to vast amounts of personal data, which raises concerns about privacy and data protection.\n","3. Job displacement: AI has the potential to displace human jobs, particularly in industries where tasks are repetitive or can be easily automated.\n","4. Explainability and transparency: A\n","-----\n","Question: What is an interaction term in linear regression and how is it used?\n","Answer: An interaction term in linear regression is a statistical term that represents the effect of the interaction between two or more predictor variables on the outcome variable. In other words, it measures the difference in the regression coefficient between the main effects of the two variables, while controlling for the effect of the other variable.\n","\n","For example, suppose we are analyzing the relationship between the amount of exercise (in minutes) and the risk of heart disease, while also considering the effect of age on the relationship. If we include an interaction term between exercise and age in the regression model, we are able to estimate the additional effect of exercise on heart disease risk beyond the effect of age. This allows us to separate the independent effects of exercise and age on heart disease risk, rather than simply combining them into a single term.\n","\n","Interaction terms are useful in AI applications where we need to understand how different factors interact with each other to produce a particular outcome. For instance, in natural language processing, we might use interaction terms to model how the meaning of a sentence changes based on the context in which it is used. In image processing, we might use interaction terms to model how the lighting and color of an image affect each other.\n","\n","In conclusion, interaction terms in linear regression are a powerful tool for analyzing the relationships between multiple variables. By modeling the interaction between two or more predictor variables, we can gain a deeper understanding of how they work together to produce a particular outcome, and make more accurate predictions in AI applications.\n","-----\n","Question: What is standardization? Under which circumstances should data be standardized?\n","Answer: 01. Standardization is the process of transforming data into a consistent format to facilitate analysis, comparison, and modeling. It is an essential step in the data preprocessing stage of machine learning, as it ensures that the data is in a suitable format for the algorithm to work with.\n","\n","02. Data should be standardized under the following circumstances:\n","\n","a. When the data is collected from different sources and in different formats, standardization helps to ensure that the data is consistent and can be easily combined.\n","\n","b. When the data contains missing values or outliers, standardization can help to handle these issues by transforming the data into a more consistent range.\n","\n","c. When the data is noisy or has a large range of values, standardization can help to reduce the noise and improve the model's performance.\n","\n","d. When the data is not normally distributed, standardization can help to transform the data into a more normal distribution, which can improve the performance of certain machine learning algorithms.\n","\n","e. When the data is categorical, standardization can help to convert the data into a numerical format, which can be easily used in machine learning algorithms.\n","\n","f. When the data is in a format that is not suitable for the specific machine learning algorithm being used, standardization can help to transform the data into a more suitable format.\n","\n","g. When the data is collected over time, standardization can help to ensure that the data is consistent and does not change over time.\n","\n","h. When the data is collected from different sources and in different formats, standardization can help to ensure that the data is consistent and can be easily combined.\n","\n","i. When the data is used for comparison or modeling, standardization can help to ensure that the data is in a consistent format, which can improve the accuracy of the model.\n","\n","j. When the data is used for prediction or forecasting, standardization can help to ensure that the data is in a consistent format, which can improve the accuracy of the prediction.\n","\n","k. When the data is used for clustering or segmentation, standardization can help to ensure that the data is in a consistent format, which can improve the accuracy of the clustering or segmentation.\n","\n","l. When the data is used for anomaly detection or outlier identification, standardization can help to ensure that the data\n","-----\n","Question: What is time- sharing system?\n","Answer: A time-sharing system is a computer system that allows multiple users to access the computer simultaneously and efficiently. In a time-sharing system, the computer's processing resources are divided into small time slices, and each user is allocated a specific time slice to execute their tasks. This allows multiple users to use the computer at the same time, reducing the overall processing time and increasing the system's utilization.\n","\n","Time-sharing systems are commonly used in mainframe computers, where many users access the computer remotely through a terminal or terminal emulator. The time-sharing system manages the allocation of processing resources among the users, ensuring that each user's tasks are executed efficiently and without interfering with the tasks of other users.\n","\n","Time-sharing systems can be further divided into two categories:\n","\n","1. Centralized time-sharing systems: In these systems, a central computer manages the allocation of processing resources among the users. The users access the central computer through a terminal or terminal emulator.\n","2. Distributed time-sharing systems: In these systems, the processing resources are distributed among multiple computers, and the users access the system through a network.\n","\n","Time-sharing systems have several advantages, including:\n","\n","1. Increased productivity: By allowing multiple users to access the computer simultaneously, time-sharing systems increase the overall productivity of the system.\n","2. Reduced waiting time: With time-sharing systems, users do not have to wait for long periods of time for their tasks to be executed, as the computer allocates processing resources efficiently among the users.\n","3. Improved system utilization: Time-sharing systems ensure that the computer's processing resources are utilized efficiently, reducing the overall cost of the system.\n","\n","However, time-sharing systems also have some disadvantages, including:\n","\n","1. Limited processing power: With multiple users accessing the computer simultaneously, the processing power of the system may be limited, leading to slower execution of tasks.\n","2. Security risks: With multiple users accessing the computer, there is a higher risk of security breaches, as users may attempt to access sensitive information or disrupt the system.\n","3. Complexity: Time-sharing systems can be complex to manage and maintain, as the system requires careful allocation of processing resources among the users.\n","\n","In summary, time-\n","-----\n","Question: Why partitioning and formatting is a prerequisite to installing an operating system?\n","Answer: 01. Partitioning is a prerequisite for installing an operating system because it allows for the creation of multiple logical drives on a single physical hard drive. This is necessary because modern operating systems require a separate partition for the operating system itself, as well as additional partitions for storing data and applications.\n","\n","Without partitioning, the operating system would not be able to distinguish between the physical hard drive and the various partitions on it, which would make it impossible to install and run the operating system correctly.\n","\n","Additionally, partitioning allows for better organization and management of the hard drive, making it easier to back up and recover data in the event of a system failure.\n","\n","In summary, partitioning is a crucial step in the installation process because it allows for the creation of separate logical drives on a single physical hard drive, which is necessary for the proper functioning of modern operating systems.\n","\n","**\n","-----\n"]}]},{"cell_type":"markdown","source":["### prompt1"],"metadata":{"id":"zgG2Jle4F9k1"}},{"cell_type":"code","source":["prompt1 = open(\"prompt1.txt\").read()\n","print(prompt1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W3lf8gsgF9XP","executionInfo":{"status":"ok","timestamp":1700456323438,"user_tz":-540,"elapsed":384,"user":{"displayName":"‍정지원[ 학부재학 / 데이터과학과 ]","userId":"01067494734646856617"}},"outputId":"28ac0f68-5a4b-4533-b8b8-450ff6006ae7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["You will be given one question and one answer. Your task is to rate the answer on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n","\n","Evaluation Criteria:\n","Factual Consistency (1-5): Does the answer contain untruthful or misleading facts that are not supported by the question?\n","\n","Evaluation Steps:\n","1. Review the question and answer provided carefully to understand the context.\n","2. Evaluate the given answer's factual consistency based on whether the information aligns with or contradicts the question.\n","3. Rate the factual consistency of the answer on a scale of 1 to 5, where 1 indicates a low level and 5 indicates a high level of consistency with the question.\n","\n","Example:\n","\n","Question:\n","\n","{{Question}}\n","\n","Answer:\n","\n","{{Answer}}\n","\n","Evaluation Form (scores ONLY):\n","\n","- Factual Consistency:\n"]}]},{"cell_type":"code","source":["from openai import OpenAI\n","import re\n","\n","client = OpenAI(\n","  api_key = \""\n",")\n","\n","avg_scores = []\n","for (q, a) in sampled_QAs:\n","  cur_prompt = prompt1.replace('{{Question}}', q).replace('{{Answer}}', a)\n","\n","  response = client.chat.completions.create(\n","      model = \"gpt-4-1106-preview\",\n","      messages = [{\"role\": \"system\", \"content\": cur_prompt}],\n","      temperature = 2,\n","      max_tokens = 10,\n","      top_p = 1, # defaults\n","      stop = None,\n","      n = 10\n","  )\n","\n","  scores = []\n","  for r in response.choices:\n","    scores += re.findall(r'\\d+', r.message.content)\n","  scores = [int(s) for s in scores]\n","  avg_score = round(sum(scores)/len(scores), 3)\n","  avg_scores.append(avg_score)\n","\n","print(avg_scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CN908d12GIpY","executionInfo":{"status":"ok","timestamp":1700458505855,"user_tz":-540,"elapsed":10186,"user":{"displayName":"‍정지원[ 학부재학 / 데이터과학과 ]","userId":"01067494734646856617"}},"outputId":"d8d630c4-3b58-493e-a4c4-4034f50581b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1.25, 4.2, 4.0, 4.5, 5.0]\n"]}]},{"cell_type":"code","source":["for i in range(len(sampled_QAs)):\n","  (q, a) = sampled_QAs[i]\n","  avg_score = avg_scores[i]\n","\n","  print('Question:\\n', q, '\\n', sep='')\n","  print('Answer:\\n', a, '\\n', sep='')\n","  print('Graded score:', avg_score, '\\n-----\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KjUuuDUBHR7u","executionInfo":{"status":"ok","timestamp":1700458518743,"user_tz":-540,"elapsed":459,"user":{"displayName":"‍정지원[ 학부재학 / 데이터과학과 ]","userId":"01067494734646856617"}},"outputId":"320521c5-56b4-41a7-f60d-0f8ee4b0bd87"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Question:\n","What is the proportion of confidence intervals that will not contain the population parameter?\n","\n","Answer:\n","80%\n","\n","The confidence interval is a measure of the uncertainty of the population parameter. The proportion of confidence intervals that will not contain the population parameter is 80%. This means that there is a 20% chance that the population parameter will be outside the range of the confidence interval.\n","\n","Graded score: 1.25 \n","-----\n","\n","Question:\n","In classification problems like logistic regression, classification accuracy alone is not considered a good measure. Why?\n","\n","Answer:\n","Accuracy is the most commonly used metric for evaluating the performance of a classifier. However, it has its drawbacks. For example, it does not provide any information about the classifier's ability to distinguish between positive and negative classes. In fact, it is possible for a classifier to achieve 100% accuracy on both the positive and negative classes. This is known as the bias-variance trade-off.\n","\n","To overcome these limitations, we use a metric called F1-score. It is a weighted average of precision and recall. Precision is the ratio of true positives to the total number of positive predictions made by the classifier. Recall is the ratio of true positives to the total number of actual positive instances.\n","\n","The F1-score is calculated as follows:\n","\n","F1 = 2 \\* (Precision \\* Recall) / (Precision + Recall)\n","\n","The F1-score ranges from 0 to 1, where 1 is the best possible score. A higher F1-score indicates better performance, while a lower score indicates poor performance.\n","\n","The F1-score is a more informative measure than accuracy because it takes into account both precision and recall. It is particularly useful when the cost of false positives and false negatives is different. For example, in medical diagnosis, a false positive diagnosis can have severe consequences, while a false negative diagnosis can lead to under-treatment.\n","\n","In summary, accuracy alone is not sufficient for evaluating the performance of a classifier. We need to use a combination of metrics, such as accuracy, precision, recall, and F1-score, to evaluate the performance of a classifier. The choice of metric depends on the specific problem and the cost of errors.\n","\n","In the context of AI, the F1-score is a particularly useful metric because it takes into account both precision and recall. It is a more informative measure than accuracy because it takes into account both the cost of false positives and false negatives.\n","\n","The F1-score is a weighted average of precision and recall, where precision and recall are calculated as follows:\n","\n","Precision = TP / (TP + FP)\n","\n","Recall = TP / (TP + FN)\n","\n","TP =\n","\n","Graded score: 4.2 \n","-----\n","\n","Question:\n","What are the advantages and disadvantages of a heap?\n","\n","Answer:\n","A heap is a data structure in which the elements are arranged in a manner such that the parent node is larger than its child nodes. This is known as a heap property. The advantages of using a heap include:\n","\n"," 1. It is easy to implement a heap.\n"," 2. It is easy to insert and delete elements from the heap.\n"," 3. It is easy to find the maximum or minimum element in a heap.\n","\n","The disadvantages of using a heap include:\n","\n"," 1. It is not suitable for all types of data.\n"," 2. It is not suitable for large data sets.\n"," 3. It is not suitable for real-time applications.\n","\n","Graded score: 4.0 \n","-----\n","\n","Question:\n","Are there any risks to extrapolation? Explain with an example when you would and would not use this.\n","\n","Answer:\n","Extrapolation is the process of using data to make predictions about an unknown future. It is a common method used in statistics and machine learning.\n","\n","Extrapolation can be useful when you have a small amount of data and need to make predictions about a larger population. For example, if you are trying to determine the average height of a population, you might use a small sample of people and extrapolate the results to the entire population.\n","\n","However, there are also risks associated with extrapolation. If you have a small amount of data, it may not accurately represent the larger population. For example, if you only have data on a small group of people, you may not be able to accurately predict the height of the entire population.\n","\n","Additionally, extrapolation can lead to overfitting, which occurs when a model is too closely fit to the training data and does not generalize well to new data.\n","\n","To avoid these risks, it is important to use caution when using extrapolation. It is important to ensure that the data you are using is representative of the larger population, and to test your model on new data to ensure that it is not overfitting.\n","\n","Example:\n","\n","Suppose we are trying to predict the number of people who will buy a particular product in a given month. We have data on the number of people who have bought the product in the past few months, but we do not have data on the number of people who will buy the product in the future. In this case, we might use extrapolation to make predictions about the future.\n","\n","However, it is important to be cautious when using extrapolation in this case, as the data we have may not accurately represent the larger population. To avoid overfitting, we might test our model on new data to ensure that it is not too closely fit to the training data.\n","\n","In conclusion, extrapolation is a useful method for making predictions about an unknown future. However, it is important to use caution when using this method, as there are risks associated with it. It is important to ensure that the data you are using is representative of the larger population, and to test your model on new data to ensure that it is not overfitting.\n","\n","Graded score: 4.5 \n","-----\n","\n","Question:\n","What is computer architecture?\n","\n","Answer:\n","Computer architecture refers to the structure and design of a computer's hardware components, including the CPU, memory, input/output devices, and storage devices. It also includes the way these components interact with each other and the operating system.\n","\n","Graded score: 5.0 \n","-----\n","\n"]}]},{"cell_type":"markdown","source":["### prompt2"],"metadata":{"id":"JS-czCsVH650"}},{"cell_type":"code","source":["prompt2 = open(\"prompt2.txt\").read()\n","print(prompt2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1p1_Vta6H8IN","executionInfo":{"status":"ok","timestamp":1700456824037,"user_tz":-540,"elapsed":4,"user":{"displayName":"‍정지원[ 학부재학 / 데이터과학과 ]","userId":"01067494734646856617"}},"outputId":"338a9fbe-a5bb-445f-94ed-82f3cd5f1953"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["You will be given one question and one answer. Your task is to rate the answer on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n","\n","Evaluation Criteria:\n","Factual Consistency (1-5): Does the answer contain untruthful or misleading facts that are not supported by the question?\n","\n","Evaluation Method:\n","Rate the factual consistency of the answer on a scale of 1 to 5, where 1 indicates a low level and 5 indicates a high level of consistency with the question.\n","- 1: Poor. The answer contains multiple inaccuracies or misleading details.\n","- 3: Fair. Some elements of the answer lack factual support or contain minor inconsistencies, but the majority aligns with the question's context.\n","- 5: Good. The answer is factually consistent, presenting information supported by the question without any misleading details.\n","\n","Example:\n","\n","Question:\n","\n","{{Question}}\n","\n","Answer:\n","\n","{{Answer}}\n","\n","Evaluation Form (scores ONLY):\n","\n","- Factual Consistency:\n"]}]},{"cell_type":"code","source":["from openai import OpenAI\n","import re\n","\n","client = OpenAI(\n","  api_key = \""\n",")\n","\n","responses = []\n","avg_scores2 = []\n","for (q, a) in sampled_QAs:\n","  cur_prompt = prompt2.replace('{{Question}}', q).replace('{{Answer}}', a)\n","\n","  response = client.chat.completions.create(\n","      model = \"gpt-4-1106-preview\",\n","      messages = [{\"role\": \"system\", \"content\": cur_prompt}],\n","      temperature = 2,\n","      max_tokens = 10,\n","      top_p = 1, # defaults\n","      stop = None,\n","      n = 10\n","  )\n","\n","  scores = []\n","  for r in response.choices:\n","    search = re.search(r'\\d+', r.message.content)\n","    if search:\n","      score = int(search.group())\n","      if score >= 1 and score <= 5:\n","        scores.append(score)\n","  if len(scores):\n","    avg_score = round(sum(scores)/len(scores), 3)\n","  else: avg_score = 0\n","  avg_scores2.append(avg_score)\n","\n","print(avg_scores2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1rlH1UQgIEF4","executionInfo":{"status":"ok","timestamp":1700459840203,"user_tz":-540,"elapsed":9526,"user":{"displayName":"‍정지원[ 학부재학 / 데이터과학과 ]","userId":"01067494734646856617"}},"outputId":"7e236216-bba4-49d3-fd16-c83a2eb6fbfd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1.222, 3.111, 3.111, 4.8, 5.0]\n"]}]},{"cell_type":"code","source":["for i in range(len(sampled_QAs)):\n","  (q, a) = sampled_QAs[i]\n","  avg_score = avg_scores2[i]\n","\n","  print('Question:\\n', q, '\\n', sep='')\n","  print('Answer:\\n', a, '\\n', sep='')\n","  print('Graded score:', avg_score, '\\n-----\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qlr2ZRQWIK8P","executionInfo":{"status":"ok","timestamp":1700459855398,"user_tz":-540,"elapsed":531,"user":{"displayName":"‍정지원[ 학부재학 / 데이터과학과 ]","userId":"01067494734646856617"}},"outputId":"706cdf47-55a3-4f5a-9020-643ae88ad558"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Question:\n","What is the proportion of confidence intervals that will not contain the population parameter?\n","\n","Answer:\n","80%\n","\n","The confidence interval is a measure of the uncertainty of the population parameter. The proportion of confidence intervals that will not contain the population parameter is 80%. This means that there is a 20% chance that the population parameter will be outside the range of the confidence interval.\n","\n","Graded score: 1.222 \n","-----\n","\n","Question:\n","In classification problems like logistic regression, classification accuracy alone is not considered a good measure. Why?\n","\n","Answer:\n","Accuracy is the most commonly used metric for evaluating the performance of a classifier. However, it has its drawbacks. For example, it does not provide any information about the classifier's ability to distinguish between positive and negative classes. In fact, it is possible for a classifier to achieve 100% accuracy on both the positive and negative classes. This is known as the bias-variance trade-off.\n","\n","To overcome these limitations, we use a metric called F1-score. It is a weighted average of precision and recall. Precision is the ratio of true positives to the total number of positive predictions made by the classifier. Recall is the ratio of true positives to the total number of actual positive instances.\n","\n","The F1-score is calculated as follows:\n","\n","F1 = 2 \\* (Precision \\* Recall) / (Precision + Recall)\n","\n","The F1-score ranges from 0 to 1, where 1 is the best possible score. A higher F1-score indicates better performance, while a lower score indicates poor performance.\n","\n","The F1-score is a more informative measure than accuracy because it takes into account both precision and recall. It is particularly useful when the cost of false positives and false negatives is different. For example, in medical diagnosis, a false positive diagnosis can have severe consequences, while a false negative diagnosis can lead to under-treatment.\n","\n","In summary, accuracy alone is not sufficient for evaluating the performance of a classifier. We need to use a combination of metrics, such as accuracy, precision, recall, and F1-score, to evaluate the performance of a classifier. The choice of metric depends on the specific problem and the cost of errors.\n","\n","In the context of AI, the F1-score is a particularly useful metric because it takes into account both precision and recall. It is a more informative measure than accuracy because it takes into account both the cost of false positives and false negatives.\n","\n","The F1-score is a weighted average of precision and recall, where precision and recall are calculated as follows:\n","\n","Precision = TP / (TP + FP)\n","\n","Recall = TP / (TP + FN)\n","\n","TP =\n","\n","Graded score: 3.111 \n","-----\n","\n","Question:\n","What are the advantages and disadvantages of a heap?\n","\n","Answer:\n","A heap is a data structure in which the elements are arranged in a manner such that the parent node is larger than its child nodes. This is known as a heap property. The advantages of using a heap include:\n","\n"," 1. It is easy to implement a heap.\n"," 2. It is easy to insert and delete elements from the heap.\n"," 3. It is easy to find the maximum or minimum element in a heap.\n","\n","The disadvantages of using a heap include:\n","\n"," 1. It is not suitable for all types of data.\n"," 2. It is not suitable for large data sets.\n"," 3. It is not suitable for real-time applications.\n","\n","Graded score: 3.111 \n","-----\n","\n","Question:\n","Are there any risks to extrapolation? Explain with an example when you would and would not use this.\n","\n","Answer:\n","Extrapolation is the process of using data to make predictions about an unknown future. It is a common method used in statistics and machine learning.\n","\n","Extrapolation can be useful when you have a small amount of data and need to make predictions about a larger population. For example, if you are trying to determine the average height of a population, you might use a small sample of people and extrapolate the results to the entire population.\n","\n","However, there are also risks associated with extrapolation. If you have a small amount of data, it may not accurately represent the larger population. For example, if you only have data on a small group of people, you may not be able to accurately predict the height of the entire population.\n","\n","Additionally, extrapolation can lead to overfitting, which occurs when a model is too closely fit to the training data and does not generalize well to new data.\n","\n","To avoid these risks, it is important to use caution when using extrapolation. It is important to ensure that the data you are using is representative of the larger population, and to test your model on new data to ensure that it is not overfitting.\n","\n","Example:\n","\n","Suppose we are trying to predict the number of people who will buy a particular product in a given month. We have data on the number of people who have bought the product in the past few months, but we do not have data on the number of people who will buy the product in the future. In this case, we might use extrapolation to make predictions about the future.\n","\n","However, it is important to be cautious when using extrapolation in this case, as the data we have may not accurately represent the larger population. To avoid overfitting, we might test our model on new data to ensure that it is not too closely fit to the training data.\n","\n","In conclusion, extrapolation is a useful method for making predictions about an unknown future. However, it is important to use caution when using this method, as there are risks associated with it. It is important to ensure that the data you are using is representative of the larger population, and to test your model on new data to ensure that it is not overfitting.\n","\n","Graded score: 4.8 \n","-----\n","\n","Question:\n","What is computer architecture?\n","\n","Answer:\n","Computer architecture refers to the structure and design of a computer's hardware components, including the CPU, memory, input/output devices, and storage devices. It also includes the way these components interact with each other and the operating system.\n","\n","Graded score: 5.0 \n","-----\n","\n"]}]},{"cell_type":"markdown","source":["Prompt2 가 더 깐깐히 채점하는 것 같아 prompt2 채택."],"metadata":{"id":"xieB92H8QfMD"}},{"cell_type":"code","source":["# jsonl 파일 작성 테스트\n","import json\n","\n","results = []\n","for i in range(len(sampled_QAs)):\n","  (q, a) = sampled_QAs[i]\n","  avg_score = avg_scores2[i]\n","\n","  result = {\n","      \"Question\": q,\n","      \"Answer\": a,\n","      \"Avg_score\": avg_score\n","  }\n","  results.append(result)\n","\n","saving_file_path = 'grades.jsonl'\n","\n","with open(saving_file_path, 'w') as file:\n","  for item in results:\n","    json.dump(item, file)\n","    file.write('\\n')"],"metadata":{"id":"MXK5KGKOQ5yH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Grading on LLaMA outputs"],"metadata":{"id":"NW7izaWQQouc"}},{"cell_type":"code","source":["# prompt2 사용\n","prompt2 = open(\"prompt2.txt\").read()\n","print(prompt2)"],"metadata":{"id":"jRfWhOHOQta8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702858896622,"user_tz":-540,"elapsed":267,"user":{"displayName":"‍정지원[ 학부재학 / 데이터과학과 ]","userId":"01067494734646856617"}},"outputId":"df518222-faa2-42ed-8925-dfe05dfc5e4f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["You will be given one question and one answer. Your task is to rate the answer on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n","\n","Evaluation Criteria:\n","Factual Consistency (1-5): Does the answer contain untruthful or misleading facts that are not supported by the question?\n","\n","Evaluation Method:\n","Rate the factual consistency of the answer on a scale of 1 to 5, where 1 indicates a low level and 5 indicates a high level of consistency with the question.\n","- 1: Poor. The answer contains multiple inaccuracies or misleading details.\n","- 3: Fair. Some elements of the answer lack factual support or contain minor inconsistencies, but the majority aligns with the question's context.\n","- 5: Good. The answer is factually consistent, presenting information supported by the question without any misleading details.\n","\n","Example:\n","\n","Question:\n","\n","{{Question}}\n","\n","Answer:\n","\n","{{Answer}}\n","\n","Evaluation Form (scores ONLY):\n","\n","- Factual Consistency:\n"]}]},{"cell_type":"code","source":["int(re.search(r'\\d+', response.choices[0].message.content).group())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r7p8SLLTVyz7","executionInfo":{"status":"ok","timestamp":1702859785671,"user_tz":-540,"elapsed":3,"user":{"displayName":"‍정지원[ 학부재학 / 데이터과학과 ]","userId":"01067494734646856617"}},"outputId":"326009f8-073f-4a26-819f-c8d927f6ee99"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["print(prompt2.replace('{{Question}}', data[0]['input']).replace('{{Answer}}', data[0]['output']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sxUEZjfOXdsr","executionInfo":{"status":"ok","timestamp":1702858902934,"user_tz":-540,"elapsed":4,"user":{"displayName":"‍정지원[ 학부재학 / 데이터과학과 ]","userId":"01067494734646856617"}},"outputId":"6d6bf0ae-5d90-4847-f9b6-6841e4c4cd7a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["You will be given one question and one answer. Your task is to rate the answer on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n","\n","Evaluation Criteria:\n","Factual Consistency (1-5): Does the answer contain untruthful or misleading facts that are not supported by the question?\n","\n","Evaluation Method:\n","Rate the factual consistency of the answer on a scale of 1 to 5, where 1 indicates a low level and 5 indicates a high level of consistency with the question.\n","- 1: Poor. The answer contains multiple inaccuracies or misleading details.\n","- 3: Fair. Some elements of the answer lack factual support or contain minor inconsistencies, but the majority aligns with the question's context.\n","- 5: Good. The answer is factually consistent, presenting information supported by the question without any misleading details.\n","\n","Example:\n","\n","Question:\n","\n","Is it always necessary to use an 80:20 ratio for the train test split?\n","\n","Answer:\n","\n"," No, it is not always necessary to use an 80:20 ratio for the train test split.\n","\n","In machine learning, the train test split is a technique used to separate the data into training and testing sets. The train test split is essential for evaluating the performance of the model. The 80:20 ratio is a common ratio used for train test splits, but it is not the only one.\n","\n","There are several ways to split the data, and the choice of the ratio depends on the specific problem and dataset. For example, if the dataset is imbalanced, i.e., one class has a large number of instances than the other, then the 80:20 ratio may not be appropriate. In such cases, a\n","\n","Evaluation Form (scores ONLY):\n","\n","- Factual Consistency:\n"]}]},{"cell_type":"code","source":["avg_scores_all = []"],"metadata":{"id":"KP32PkdWdPkw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from openai import OpenAI\n","from tqdm import tqdm\n","import re\n","\n","client = OpenAI(\n","  api_key = \"api_key\"\n",")\n","\n","idx = len(avg_scores_all)\n","for d in tqdm(data[idx:]):\n","  cur_prompt = prompt2.replace('{{Question}}', d['input']).replace('{{Answer}}', d['output'])\n","\n","  response = client.chat.completions.create(\n","      model = \"gpt-4-1106-preview\",\n","      messages = [{\"role\": \"system\", \"content\": cur_prompt}],\n","      temperature = 2,\n","      max_tokens = 10,\n","      top_p = 1, # defaults\n","      stop = None,\n","      n = 10\n","  )\n","\n","  scores = []\n","  for r in response.choices:\n","    search = re.search(r'\\d+', r.message.content)\n","    if search:\n","      score = int(search.group())\n","      if score >= 1 and score <= 5:\n","        scores.append(score)\n","  if len(scores):\n","    avg_score = round(sum(scores)/len(scores), 3)\n","  else: avg_score = 0\n","  avg_scores_all.append(avg_score)"],"metadata":{"id":"MduF1Zz2SW_c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702859008300,"user_tz":-540,"elapsed":78294,"user":{"displayName":"‍정지원[ 학부재학 / 데이터과학과 ]","userId":"01067494734646856617"}},"outputId":"5ac1614a-3b17-4c57-b31d-c97acb0f3467"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 51/51 [01:17<00:00,  1.53s/it]\n"]}]},{"cell_type":"code","source":["len(avg_scores_all)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B_gUVBxVZNhK","executionInfo":{"status":"ok","timestamp":1702223269167,"user_tz":-540,"elapsed":326,"user":{"displayName":"‍정지원[ 학부재학 / 데이터과학과 ]","userId":"01067494734646856617"}},"outputId":"fd163226-4f70-4360-e6c1-377c2369eb86"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["481"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["len(data[len(avg_scores_all):])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6zaLUvZfdTGz","executionInfo":{"status":"ok","timestamp":1702223270814,"user_tz":-540,"elapsed":3,"user":{"displayName":"‍정지원[ 학부재학 / 데이터과학과 ]","userId":"01067494734646856617"}},"outputId":"80024b49-367a-4f39-e345-f6d2fae6fbe0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["len(data[1:])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pmT2fRLXde_t","executionInfo":{"status":"ok","timestamp":1702859025863,"user_tz":-540,"elapsed":4,"user":{"displayName":"‍정지원[ 학부재학 / 데이터과학과 ]","userId":"01067494734646856617"}},"outputId":"8e113187-0b26-4c77-a6cf-6250c7e5bb46"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["50"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["results = []\n","for (i, d) in enumerate(data):\n","    q = d['input']\n","    a = d['output']\n","    avg_score = avg_scores_all[i]\n","\n","    result = {\n","        \"Question\": q,\n","        \"Answer\": a,\n","        \"Avg_score\": avg_score\n","    }\n","    results.append(result)\n","\n","saving_file_path = 'grading_got_results.jsonl'\n","\n","with open(saving_file_path, 'w') as file:\n","  for item in results:\n","    json.dump(item, file)\n","    file.write('\\n')"],"metadata":{"id":"1ddcB2UDZ2rB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["comparison\n"],"metadata":{"id":"sgx7sowQsdmW"}},{"cell_type":"code","source":["%cd '/content'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RiGxFUYxsnN4","executionInfo":{"status":"ok","timestamp":1700550317518,"user_tz":-540,"elapsed":3,"user":{"displayName":"‍정지원[ 학부재학 / 데이터과학과 ]","userId":"01067494734646856617"}},"outputId":"7fe8f91f-85d8-4bbf-8f36-95627e819299"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LJaM5MIRs4QQ","executionInfo":{"status":"ok","timestamp":1700550380913,"user_tz":-540,"elapsed":397,"user":{"displayName":"‍정지원[ 학부재학 / 데이터과학과 ]","userId":"01067494734646856617"}},"outputId":"e9df854b-ac6a-482d-9ee8-f44c52ce0bee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["drive  grading_results_base.jsonl  grading_results.jsonl  sample_data\n"]}]},{"cell_type":"code","source":["file1 = 'grading_results.jsonl'\n","\n","grading1 = []\n","with open(file1, 'r', encoding='utf-8') as file:\n","    for line in file:\n","        json_obj = json.loads(line.strip())\n","        grading1.append(json_obj)\n","\n","total_scores1 = 0\n","for g1 in grading1:\n","  total_scores1 += g1['Avg_score']\n","print(total_scores1/len(grading1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GDZvDMvdscv2","executionInfo":{"status":"ok","timestamp":1702056906110,"user_tz":-540,"elapsed":339,"user":{"displayName":"‍정지원[ 학부재학 / 데이터과학과 ]","userId":"01067494734646856617"}},"outputId":"87dbc133-2928-471a-dc7d-3dafcb74b1e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3.9114261954261953\n"]}]},{"cell_type":"code","source":["file2 = 'grading_results_baseline.jsonl'\n","\n","grading2 = []\n","with open(file2, 'r', encoding='utf-8') as file:\n","    for line in file:\n","        json_obj = json.loads(line.strip())\n","        grading2.append(json_obj)\n","\n","total_scores2 = 0\n","for g2 in grading2:\n","  total_scores2 += g2['Avg_score']\n","print(total_scores2/len(grading2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xP-L96EStLnX","executionInfo":{"status":"ok","timestamp":1702223283155,"user_tz":-540,"elapsed":285,"user":{"displayName":"‍정지원[ 학부재학 / 데이터과학과 ]","userId":"01067494734646856617"}},"outputId":"a96c000b-4757-4096-c8f2-48fa2b2ec0bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["4.0077047817047795\n"]}]},{"cell_type":"code","source":["file1 = 'grading_got_results.jsonl'\n","\n","grading1 = []\n","with open(file1, 'r', encoding='utf-8') as file:\n","    for line in file:\n","        json_obj = json.loads(line.strip())\n","        grading1.append(json_obj)\n","\n","total_scores1 = 0\n","for g1 in grading1:\n","  total_scores1 += g1['Avg_score']\n","print(total_scores1/len(grading1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XPqSY1ujTzTv","executionInfo":{"status":"ok","timestamp":1702859994772,"user_tz":-540,"elapsed":4,"user":{"displayName":"‍정지원[ 학부재학 / 데이터과학과 ]","userId":"01067494734646856617"}},"outputId":"b0fdd96c-09c2-4d91-a3e0-ec08b94186ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3.2877799999999997\n"]}]}]}
